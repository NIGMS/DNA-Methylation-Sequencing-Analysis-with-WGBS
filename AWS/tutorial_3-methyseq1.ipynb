{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd00134-ed34-43cb-9b43-d65783d386a8",
   "metadata": {},
   "source": [
    "# The WGBS data analysis tutorial 3 - nf-core/methylseq\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we'll introduce the basics of Nextflow, the nf-core/methylseq pipeline and how to use them to run our example dataset in the Vertex AI notebook local environment:\n",
    "- [Nextflow introduction](#Nextflow-introduction) - Introduction to Nextflow, write a simple Nextflow script\n",
    "- [nf-core](#nf-core) - Introduction to nf-core\n",
    "- [nf-core/methylseq](#nf-core/methylseq) - Install and run the pipeline\n",
    "- [Understanding nf-core/methylseq Output](#Understanding-nf-core/methylseq-Output) - compare to the Bismark workflow\n",
    "\n",
    "<img src=\"images/notebook3.png\" width=\"700\" />\n",
    "\n",
    "__[Nf-core/methylseq](https://nf-co.re/methylseq/1.6.1)__ is a bioinformatics analysis pipeline used for Methylation (Bisulfite) sequencing data. It pre-processes raw data from FastQ inputs, aligns the reads and performs extensive quality-control on the results.\n",
    "\n",
    "The pipeline is built using __[Nextflow](https://www.nextflow.io/)__, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with Docker containers making installation trivial and results highly reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12a028",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "* **Introduce Nextflow:** Understand the basic concepts of Nextflow, including processes, channels, and configuration files.  Learn to write a simple Nextflow script (\"Hello World\").\n",
    "\n",
    "* **Introduce nf-core:** Understand what nf-core is and how it provides a collection of curated bioinformatics pipelines. Learn how to list available nf-core pipelines.\n",
    "\n",
    "* **Utilize nf-core/methylseq:** Learn to install and run the nf-core/methylseq pipeline for analyzing bisulfite sequencing data. Understand the pipeline's parameters and how to configure them.  Learn how to create a samplesheet for the pipeline.\n",
    "\n",
    "* **Interpret nf-core/methylseq Output:** Understand the structure of the nf-core/methylseq output directories (`work` and `results`). Identify key output files for downstream analysis (e.g., methylation calls in bedGraph format).  Compare the results to those obtained from a similar workflow (Bismark).\n",
    "\n",
    "* **Manage Nextflow Execution:** Learn how to use Nextflow's `-resume` option to modify and resume pipeline execution. Understand the priority of different Nextflow configuration sources.\n",
    "\n",
    "* **Practical Application:** Apply the learned concepts by running nf-core/methylseq on a provided example dataset.  Learn how to manage resources (CPU and memory) within the pipeline.\n",
    "\n",
    "* **Data Management:** Learn how to transfer results to Google Cloud Storage and clean up intermediate files.\n",
    "\n",
    "* **Reinforce Learning:**  Consolidate understanding through a quiz at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069afdd",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**1. Data:**\n",
    "\n",
    "* **FastQ files:** These are the input sequencing data. The notebook shows using data from `Tutorial_1`, which must be prepared beforehand.\n",
    "* **Reference Genome:** A FASTA file of the reference genome (mouse chromosome 6 in this example) is required for read alignment. This too must be prepared and accessible from within the Vertex AI Workbench environment (`Tutorial_1/ref_genome/Mus_musculus.GRCm39.dna.chromosome.6.fa`).\n",
    "\n",
    "**2. Software Dependencies:**\n",
    "\n",
    "* **Nextflow:** The notebook itself installs Nextflow using `mamba`.\n",
    "* **Mamba:** a package manager, used for installing Nextflow and other potential tools.\n",
    "* **Docker:** Required by the `-profile docker` parameter within the `nextflow run` command, for running the nf-core/methylseq pipeline.  This is already provided in the pre-configured environment but still needs to be specified within the `nextflow run` command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e7aab-a524-43d2-9277-0ad158bc6d8c",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "<img src=\"images/3_nextflow_logo.png\" width=\"300\" />\n",
    "\n",
    "### Nextflow introduction\n",
    "Please visit https://www.nextflow.io/docs/latest/index.html for more information about Nextflow. \n",
    "\n",
    "Nextflow is a reactive workflow framework and a programming DSL (Nextflow defaults to DSL 2 if no version is specified explicitly) that eases the writing of data-intensive computational pipelines. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations. Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model. \n",
    "\n",
    "#### Basic concept\n",
    "\n",
    "In practice a Nextflow pipeline script is made by joining together different **[processes](https://www.nextflow.io/docs/latest/process.html)**. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.). Processes are executed independently and are isolated from each other, i.e. they do not share a common (writable) state. The only way they can communicate is via asynchronous FIFO queues, called __[channels](https://www.nextflow.io/docs/latest/channel.html)__ in Nextflow. Any process can define one or more channels as input and output.\n",
    "\n",
    "Nextflow interacts with many different files to have a proper working workflow:\n",
    "\n",
    "- __Main file__: The main file is a `.nf` file that holds the processes and channels describing the input, output, a shell script of your commands and workflow\n",
    "- __Config file__: Pipeline default configuration properties are defined in a file named `nextflow.config` in the pipeline execution directory.The `.config` file contains parameters, and multiple profiles. Each profile can contain a different __executor__ type (e.g. LS API, conda, docker, etc.), memory or machine type, output directory, working directory and more. \n",
    "- __Docker file (optional)__: Contains dependencies and environments that is needed for the Nextflow workflow to run.\n",
    "- __Schema file (optional)__: Schema files are optional and are structured .json files that contain information about the usage and commands that your workflow will execute.You might have seen this when you run a command along with the flag '--help'.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  When using the nf-core/methylseq, or other nf-core pipelines, users do not need to create any of these files except the config file. And besides Docker, users can select other software dependency management tools (Docker, Singularity, Conda). \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496a7cc-fe0c-496d-8128-f9b32373e649",
   "metadata": {},
   "source": [
    "#### Install Nextflow\n",
    "\n",
    "The commands below will install Nextflow using `mamba` in this notebook instance.\n",
    "\n",
    "Now, let's install `Nextflow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956aa30-d9b8-423c-a5b1-2a4aef8adcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mamba install -c bioconda nextflow -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341f76e-f99b-4b14-b548-64ea637a3b96",
   "metadata": {},
   "source": [
    "#### Your first Nextflow script: 'Hello World'\n",
    "\n",
    "- Create a `hello.nf` file in the terminal\n",
    "- Be sure to include _#!/usr/bin/env nextflow_ and _nextflow.enable.dsl=2_ at the top of your script\n",
    "- Add a process that is named `sayHello`. This process will catch the input as a string (or use the pre-defined str 'Hello World' in no input provided) and write the string to a file then print the content of that file. The file will be in the current working directory `<work>`\n",
    "- At the end write the order of your workflow\n",
    "    - For our example we are running the sayHello process and the final output is printed by the `view` operator\n",
    "\n",
    "It should look something like this:\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2 \n",
    "\n",
    "params.str = 'Hello World'\n",
    "\n",
    "  process sayHello {\n",
    "  input:\n",
    "  val str\n",
    "\n",
    "  output:\n",
    "  stdout\n",
    "\n",
    "  \"\"\"\n",
    "  echo $str > hello.txt\n",
    "  cat hello.txt\n",
    "  \"\"\"\n",
    "}\n",
    "workflow {\n",
    "  sayHello(params.str) | view\n",
    "}\n",
    "```\n",
    "\n",
    "Execute the script by entering the following command. It will output how the process is executed and the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ab9c6-405b-40a0-85d4-e7eebd401b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow run docs/hello.nf -work-dir \"Tutorial_3/work\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082f369-7eb0-432c-be96-43001b8f8c03",
   "metadata": {},
   "source": [
    "There is no output directory, however, there should be an intermediate output file `hello.txt` generated in the `Tutorial_3/work` directory. If the work directory is not defined by `-work-dir` or `-w`, then Nextflow will created a directory called `work` automatically for its intermediate outputs. The sub-directory name should be the one in front of the process in the brackets. For example, Tutorial_3/work/18/e5f57536aff7b3193cc3b6d8175266/hello.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feabd57-db77-4a86-83a7-e6aea6e3cc32",
   "metadata": {},
   "source": [
    "#### Pipeline parameters\n",
    "Pipeline parameters are simply declared by prepending to a variable name the prefix `params`, separated by dot (`.`) character. Their value can be specified on the command line by prefixing the parameter name with a double dash character, i.e. `--paramName`\n",
    "\n",
    "For example, we can run the previous script specifying a different input string parameter (`--str`), as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df2b25-d720-4408-80f7-a0de19dcd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow run docs/hello.nf --str \"Good morning!\" -work-dir \"Tutorial_3/work\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e806197-e319-49a2-9cbb-a8c8ea986e01",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "\n",
    "The configuration file can be used to define which executor to use, the processâ€™s environment variables, pipeline parameters etc. When a pipeline script is launched, Nextflow looks for configuration files in multiple locations. Since each configuration file can contain conflicting settings, the sources are ranked to decide which settings to are applied. All possible configuration sources are reported below, listed in order of **priority**:\n",
    "\n",
    "- Parameters specified on the command line (`--something value`)\n",
    "- Parameters provided using the `-params-file` option\n",
    "- Config file specified using the `-c` my_config option\n",
    "- The config file named `nextflow.config` in the current directory\n",
    "- The config file named `nextflow.config` in the workflow project directory\n",
    "- The config file `$HOME/.nextflow/config`\n",
    "- Values defined within the pipeline script itself (e.g. `main.nf`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369a17a-78ef-4a2a-bff0-57a972ed2a6f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <i class=\"fa fa-lightbulb-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Tip: </b> Modify and resume</div>\n",
    "<p>Nextflow keeps track of all the processes executed in your pipeline. If you modify some parts of your script, only the processes that are actually changed will be re-executed. The execution of the processes that are not changed will be skipped and the cached result used instead. After saving the changes (.nf or .config), save the file with the same name and execute it by adding the <code>-resume</code> option to the command line. But remember if you wish to use the `-resume` function, then the previous runs should not be cleaned up, especially the work directories.\n",
    "\n",
    "This helps a lot when testing or modifying part of your pipeline without having to re-execute it from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be77e9-a77a-45ff-adee-0a39d89e4feb",
   "metadata": {},
   "source": [
    "## <a name=\"nf-core\" /><img src = \"images/3_nf-core-logo.png\" width= \"300\" />\n",
    "\n",
    "__[nf-core](https://nf-co.re/)__ is a community effort to collect a curated set of analysis pipelines built using Nextflow. And now, 69 pipelines (Sep 2022) are currently available as part of nf-core. We can use the following command to check the list of available pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b667e5-9c43-4cdf-b639-4450ae771b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run nfcore/tools list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb36f8b-9c0a-410f-b7dc-48a0bca96372",
   "metadata": {},
   "source": [
    "## nf-core/methylseq\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "\"**nf-core/methylseq** is a bioinformatics analysis pipeline used for Methylation (Bisulfite) sequencing data. It pre-processes raw data from FastQ inputs, aligns the reads and performs extensive quality-control on the results. The pipeline is built using Nextflow, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with Docker containers making installation trivial and results highly reproducible.\" - [https://github.com/nf-core/methylseq](https://github.com/nf-core/methylseq)\n",
    "\n",
    "The basic steps from this pipeline are similar to the Bismark pipeline introduced in tutorial 1, but with more quality control steps:  \n",
    "> <img src = \"images/3_methylseq_steps.png\" width = \"801\" />\n",
    "\n",
    "#### Quick Start\n",
    "\n",
    "To run nf-core/methylseq, you need to:\n",
    "- [X] Install Nextflow\n",
    "- [X] Install any of **Docker**, Singularity, Podman, Shifter or Charliecloud for full pipeline reproducibility (Docker comes with the instance, no need to install)\n",
    "- [ ] Download the pipeline and test it on a minimal dataset with a single command\n",
    "- [ ] Start running your own analysis!\n",
    "\n",
    "A typical methylseq command looks like this: `nextflow run nf-core/methylseq --input '*_R{1,2}.fastq.gz' -profile docker`. There are many parameters that you can add or change to the pipeline. Please see a detailed list of these options by using the `--help` flag (the output directory only save some pipeline info here, can be delete later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d57717-f940-4615-bc3e-fab00cba8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow run nf-core/methylseq -r 2.6.0 --help --outdir \"Tutorial_3/help_dir\" -work-dir \"Tutorial_3/work\"\n",
    "! rm -rf Tutorial_3/help_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee5940-ead3-45a8-8abe-949024753f60",
   "metadata": {},
   "source": [
    "#### Create a samplesheet (located in Tutorial_3) to provide all sample information\n",
    "\n",
    "**Format:**    \n",
    "sample, fastq1, fastq2    \n",
    "sample1,sample1_R1.fastq.gz,sample1_R2.fastq.gz    \n",
    "control1,control1_R1.fastq.gz,control1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acefe12-30c8-482a-a806-22c92d902710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame by lists of dicts.\n",
    "import pandas as pd\n",
    " \n",
    "# Initialize data to lists.\n",
    "data = [{'sample': 'SRX202087_serum', 'fastq_1': 'Tutorial_1/fastq/SRX202087_R1.fastq.gz', 'fastq_2': 'Tutorial_1/fastq/SRX202087_R2.fastq.gz'},\n",
    "        {'sample': 'SRX202088_2i', 'fastq_1': 'Tutorial_1/fastq/SRX202088_R1.fastq.gz', 'fastq_2': 'Tutorial_1/fastq/SRX202088_R2.fastq.gz'},\n",
    "       {'sample': 'SRX271141_serum', 'fastq_1': 'Tutorial_1/fastq/SRX271141_R1.fastq.gz', 'fastq_2': 'Tutorial_1/fastq/SRX271141_R2.fastq.gz'},\n",
    "       {'sample': 'SRX271142_2i', 'fastq_1': 'Tutorial_1/fastq/SRX271142_R1.fastq.gz', 'fastq_2': 'Tutorial_1/fastq/SRX271142_R2.fastq.gz'}]\n",
    " \n",
    "# Creates DataFrame.\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    "# Print the data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9c1f2-b2f7-43b0-bba1-e90645fada69",
   "metadata": {},
   "source": [
    "Export dataframe to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb10e6e-39e9-4ae9-9cc2-aeefe36d3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Tutorial_3/samplesheet.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16206bbb-aeaf-4fa0-bd2c-5f4eeab95f9b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-pencil\" aria-hidden=\"true\"></i>\n",
    "<b>Note: output directory should NOT exist yet.</b> If not defined, the default output directory is <b>'results'</b>, and the output directory cannot exist before running the pipeline, or you will get an error message. Remove the output directory if this is not your first time to run it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a989a12-9fd3-492c-86ae-bda7a23ea2a6",
   "metadata": {},
   "source": [
    "#### Run nf-core/methylseq pipeline with your own data\n",
    "\n",
    "In this step, we will run the example dataset we downloaded in tutorial 1 using methylseq. If you haven't downloaded the dataset and its reference, please read the instructions [\"Importing the example dataset\"](tutorial_1-bismark.ipynb#Importing-the-example-dataset).\n",
    "\n",
    "The parameters that need to be provided are:\n",
    "- `--fasta` - the reference sequences, usually the reference genome. In our example, we use the sequences from mouse chromosome 6 to be the reference sequence. You can use `--genome` instead to provide the iGenomes reference, such as \"GRCh38\" for human. For all the ready-to-use genomes, please visit https://support.illumina.com/sequencing/sequencing_software/igenome.html.\n",
    "- `--input` - to specify the location of your input fastq files. Please note the following requirement:\n",
    "    1. The path must be enclosed in quotes\n",
    "    2. The path must have at least one * wildcard character\n",
    "    3. When using the pipeline with paired end data, the path must use {1,2} notation to specify read pairs. Note, in Jupyter notebooks, you need to replace the {} to \\[\\] for the system to interpret the name correctly\n",
    "- `--outdir` - where to save the pipeline output files. The default: `./results`. In our example, we change the output directory to `Tutorial_3/methylseq_results`. **Note:** this directory cannot exist before the running of the pipeline, or you will get error saying it already exists. So you can remove the directory first using `rm -rf` if this is not the first time you run this step.\n",
    "- `-w` - working directory\n",
    "- `max_cpu` and `max_memory` - these parameters act as a cap, to prevent Nextflow from going over what is possible on your system. For the compute node default settings we selected when creating the notebook, the maximum CPUs we can use is 4, and maximum memory is 15GB\n",
    "\n",
    "Now, run nf-core/methylseq pipeline with these settings (it will take ~30 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96daf9-cb78-452b-b9f6-a875c6a66faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf Tutorial_3/methylseq_results\n",
    "\n",
    "! nextflow run nf-core/methylseq -r 2.6.0 -profile docker -work-dir 'Tutorial_3/work' \\\n",
    "    --fasta 'Tutorial_1/ref_genome/Mus_musculus.GRCm39.dna.chromosome.6.fa' \\\n",
    "    --input 'Tutorial_3/samplesheet.csv' \\\n",
    "    --outdir 'Tutorial_3/methylseq_results' \\\n",
    "    --max_cpus 4 \\\n",
    "    --max_memory 12.GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea3284-ab7e-4e0c-8c87-22ebda46bc5e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-pencil\" aria-hidden=\"true\"></i>\n",
    "<b>Note: failed process preseq</b>. You may notice that the process <code>preseq</code> fails in this test run. Preseq is used to predict and estimate the complexity of a genomic sequencing library. The preseq process often fails, but it can be ignored since it does not affect the final output files. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc98ab-09fa-4a4f-a6d9-edbf28c558a6",
   "metadata": {},
   "source": [
    "## Understanding nf-core/methylseq Output\n",
    "\n",
    "#### Two output directories\n",
    "\n",
    "Unless otherwise specified, there will be two output directories from the nf-core/methylseq pipeline: `work` and `results`. `work` is the working directory, where the intermediate files will be saved. `results` is where the outputs from the pipeline will be saved and it can be changed using the `--outdir` in the command. In our example above, the work directory is `Tutorial_3/work` and the results directory is `Tutorial_3/methylseq_results`.\n",
    "\n",
    "The results from each step will be in separate sub-directories in the results directory. In our example, they are saved to `Tutorial_3/methylseq_results`, and the all the resulting directories/files are saved as shown in the figure below:\n",
    "\n",
    "> <img src=\"images/3_methylseq_output.png\" alt=\"nf-core/methylseq output\" width=\"800\"/>\n",
    "\n",
    "More detailed explanation can be found in the `Tutorial_3/methyseq_results/pipeline_info`. You might need to press `Trust HTML` at the top of the file or use `Ctrl` and click the links inside this HTML to open the web pages in the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f6f9a-1bde-4af0-8fd5-ec3c4f9b43b0",
   "metadata": {},
   "source": [
    "#### Compare the results from two pipelines\n",
    "\n",
    "For example, you can find the summary of the Bismark step in the `results/bismark_summary`, which should be very similar to the Bismark summary we generated ([step 8](tutorial_1-bismark.ipynb#Step-8.-Generate-report-and-summary)) from running the Bismark workflow in **tutorial 1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32908e-3779-4267-80e9-0dc4ab4cf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bismark results from this tutorial - methylseq\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='Tutorial_3/methylseq_results/bismark/summary/bismark_summary_report.html', width=1000, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07fd0a-6675-40b8-a700-47ebef503bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bismark results from tutorial 1 - step 8\n",
    "IFrame(src='Tutorial_1/bismark/bismark_summary_report.html', width=1000, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d387a-b983-4254-b972-55e914d17ae8",
   "metadata": {},
   "source": [
    "#### Files for downstream analysis\n",
    "\n",
    "As mentioned in the tutorial 1, the final output will be used for other downstream analysis can be found in the output directory:\n",
    "- `bismark_methylation_calls/bedGraph`   \n",
    "Methylation statuses in bedGraph format, with 0-based genomic start and 1- based end coordinates.\n",
    "- `bismark_methylation_calls/methylation_coverage`   \n",
    "Coverage text file summarizing cytosine methylation values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6f7df-0ac1-47b0-8c77-cf3803901ae9",
   "metadata": {},
   "source": [
    "## Terms & Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3894b-02ce-43f0-9ae7-6f2effc03217",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jupytercards --quiet\n",
    "from jupytercards import display_flashcards\n",
    "display_flashcards('../quiz_files/f3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da539854-858e-46f8-91f8-fdcb3aa157d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jupyterquiz --quiet\n",
    "from jupyterquiz import display_quiz\n",
    "display_quiz('../quiz_files/q3.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551743e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This tutorial provided a comprehensive introduction to Nextflow and its application in analyzing WGBS data using the nf-core/methylseq pipeline within the SageMaker notebook instance.  We began with a basic Nextflow script to illustrate core concepts like processes and channels, highlighting the pipeline's flexibility and reproducibility facilitated by Docker containers.  Subsequently, we explored the nf-core/methylseq pipeline, detailing its workflow and execution using a sample dataset, comparing its output to the Bismark workflow from a previous tutorial.  The tutorial concluded by demonstrating how to manage output files, emphasizing the importance of transferring relevant results to cloud storage and cleaning up intermediate files for efficient resource management.  The included quiz further reinforces the key concepts learned throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1924f8f8-d080-44ce-ad21-42914df65110",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Transfer useful results back to an Amazon S3 bucket, and remove intermediate files. In this tutorial, the most important output files are the methylation profiles (.bedgraph.gz) of each sample. We will use these data to identify differentially methylated regions using metilene in the next tutorial.\n",
    "\n",
    "For example, you can upload/copy all the .bedgraph.gz files to the bucket you created in Cloud Storage by using:     \n",
    "`! aws s3 cp Tutorial_3/methylseq_results/bismark_methylation_calls/bedGraph/*.bedGraph.gz s3://BUCKET_NAME/methylseq_results`\n",
    "\n",
    "You can also delete the whole directory with all the files generated in this notebook using:  \n",
    "`! rm -rf Tutorial_3/methylseq_results`\n",
    "\n",
    "**Don't forget:** after finish running the notebook, stop the notebook in Vertex AI Workbench to avoid cost accumulation."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
