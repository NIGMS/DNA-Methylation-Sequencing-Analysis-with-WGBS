{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe8280a-cc0b-41b1-a18b-3f43131ddb58",
   "metadata": {},
   "source": [
    "# The WGBS data analysis tutorial 4 \n",
    "# Run nf-core/methylseq using AWS Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcb1bb-572d-41d2-906e-c06813203765",
   "metadata": {},
   "source": [
    "## Overview\n",
    "For real-world datasets, the sequence file sizes are usually too large to process using a single virtual machine (SageMaker notebook), or take a long time. In this tutorial, we will show how to run a nf-core/methyseq pipeline to process WGBS data using the AWS Batch.  \n",
    "\n",
    "<img src=\"images/notebook4.png\" width=\"900\" />\n",
    "\n",
    "The **[AWS Batch](https://www.nextflow.io/docs/latest/aws.html#aws-batch)** is a managed compute service that allows the execution of containerized workloads in the Amazon Web Services (AWS) infrastructure. It provides a simple way to execute a series of containerized tasks on AWS. The most common use case when using AWS Batch is to run an existing tool or custom script that reads and writes files, typically to and from Amazon Simple Storage Service (S3). Nextflow provides built-in support for AWS Batch, which allows the seamless deployment of a Nextflow pipeline in the cloud, offloading the process executions through the AWS service. AWS Batch can run independently over hundreds or thousands of these files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f901a82",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* **Understand and utilize AWS Batch for large-scale WGBS data analysis:** Learn how to overcome limitations of single notebook instances by leveraging AWS Batch's managed compute service for processing large datasets.\n",
    "\n",
    "* **Set up a Nextflow IAM role and configure notebook permissions:** Learn to create and configure an IAM role with the necessary permissions to interact with AWS services (Batch, S3) from a Jupyter Notebook environment in SageMaker. This includes understanding and applying the principle of least privilege and enabling required service endpoints.\n",
    "\n",
    "* **Configure Nextflow for AWS Batch execution:** Learn to create and modify a Nextflow configuration file to specify AWS Batch as the executor, define AWS region, compute environment (including instance type, spot/on-demand settings, and resource allocation) working directory, and output directory in Amazon S3. Understand the implications of choosing different instance types and their resource implications, including understanding the cost-effectiveness of spot instances.\n",
    "\n",
    "* **Run the nf-core/methylseq pipeline on AWS Batch:** Learn to execute the nf-core/methylseq pipeline using AWS Batch, including specifying pipeline version, profiles, and config files. Understand the differences in runtime between small test datasets and larger real-world datasets.\n",
    "\n",
    "* **Process real-world WGBS datasets using nf-core/methylseq and AWS Batch:** Gain practical experience downloading and processing a real-world WGBS dataset using the `fasterq-dump` tool and creating a samplesheet for the pipeline.\n",
    "\n",
    "* **Troubleshoot large-scale pipeline execution:** Learn to identify and resolve common issues such as out-of-memory errors by adjusting instance types and memory allocation within the Nextflow config file and AWS Batch settings. Understand how to optimize pipeline configurations for resource usage.\n",
    "\n",
    "* **Interpret pipeline execution reports:** Learn to use the pipeline's execution timeline and report to understand resource utilization and identify potential bottlenecks for optimization.\n",
    "* **Manage Amazon S3 buckets:** Learn to create and manage Amazon S3 buckets for storing input data and pipeline outputs. Understand how to grant appropriate permissions to the IAM role using IAM policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdedb49",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* **Mamba:** Used for package management to install `nextflow` and `sra-tools`.\n",
    "* **Nextflow:**  The workflow management system used to run the nf-core/methylseq pipeline.\n",
    "* **nf-core/methylseq:** The bioinformatics pipeline for processing WGBS data.  Specific version is mentioned (e.g., `-r 2.4.0` and `-r 2.6.0`).\n",
    "* **Amazon S3 Bucket:** An Amazon S3 bucket is crucial for storing input data, intermediate files, and results due to the significant size of WGBS datasets. The notebook guides you through its creation. The notebook utilizes the bucket, with subdirectories like `workdir` and `output`,  for data storage when using AWS Batch or other processing services.\n",
    "* **Sample Sheet:** A CSV file containing information about the samples (fastq files).  The notebook creates this from example SRA data or needs a user-provided one for larger studies.\n",
    "* **Reference Genome:**  The notebook uses GRCh38 (human genome) as an example, but another reference genome can be specified.  The correct files must be available or the pipeline must know where to find them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e26118",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8af02",
   "metadata": {},
   "source": [
    "### AWS Batch Setup\n",
    "\n",
    "AWS Batch will create the needed permissions, roles and resources to run Nextflow in a serverless manner. You can set up AWS Batch manually or deploy it **automatically** with a stack template. The Launch Stack button below will take you to the cloud formation create stack webpage with the template with required resources already linked. \n",
    "\n",
    "If you prefer to skip manual deployment and deploy automatically in the cloud, click the Launch Stack button below. For a walkthrough of the screens during automatic deployment please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/HowToLaunchAWSBatch.md). The deployment should take ~5 min and then the resources will be ready for use. \n",
    "\n",
    "[![Launch Stack](images/LaunchStack.jpg)](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=aws-batch-nigms&templateURL=https://nigms-sandbox.s3.us-east-1.amazonaws.com/cf-templates/AWSBatch_template.yaml )\n",
    "\n",
    "\n",
    "Before begining this tutorial, if you do not have required roles, policies, permissions or compute environment and would like to **manually** set those up please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/AWS-Batch-Setup.md) to set that up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99e39f",
   "metadata": {},
   "source": [
    "### Create an Amazon S3 Bucket\n",
    "\n",
    "You can create a customized bucket to use to store the output from the pipeline. Bucket names must be **globally unique** across all AWS accounts, including those outside of your organization. \n",
    "\n",
    "<b>Note:</b> We will use bucket name \"nextflow-bucket-test\" as an example in the following steps, but you need to create your own bucket to store the data. Please replace it with your own bucket name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bc8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a S3 bucket\n",
    "! aws s3 mb nextflow-bucket-test # replace it with your own bucket name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed6116",
   "metadata": {},
   "source": [
    "### Install Nextflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26432290-b8e1-4fc6-882c-c23b9dede729",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mamba install -c bioconda nextflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow self-update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32040353-96da-49c8-8f1e-b4e8213ce2c3",
   "metadata": {},
   "source": [
    "### Create and modify your own config file to include a 'aws' profile block\n",
    "\n",
    "The config file allows Nextflow to utilize executors like Google Batch. Below is an example config file to run a Nextflow job using AWS Batch:  \n",
    "```bash\n",
    "plugins {\n",
    "    id 'nf-amazon'\n",
    "}\n",
    "\n",
    "profiles {\n",
    "    aws {\n",
    "        process {\n",
    "            executor = 'awsbatch'\n",
    "            queue = 'nextflow-batch-job-queue'\n",
    "            container = 'nfcore/methylseq'\n",
    "            \n",
    "        }\n",
    "        workDir = 's3://nextflow-bucket-test/nextflow_env/'\n",
    "        params.outdir = 's3://nextflow-bucket-test/nextflow_output/' \n",
    "        \n",
    "        fusion.enabled = true\n",
    "        wave.enabled = true\n",
    "        aws.region = 'us-east-1'\n",
    "\n",
    "    }\n",
    "}\n",
    "```  \n",
    "There are some fields that you need to define or pay attention to:\n",
    "- **Executor**. To run the job using AWS Batch, the executor must be defined here using: `process.executor = 'awsbatch'` \n",
    "- **Data storage**. For a full-scale dataset, make sure you create the bucket ahead of time and a directory in your specified bucket to store the input, output, and intermediate files. The `workDir` define the working directory for intermediate file to store. You can also define the input files and output path using the parameter `params.input` and `params.outdir` to specify your working directory bucket and output directory bucket\n",
    "    - If not defined, the work directory and output directory with be in your local notebook directory named `work`, and `results`. This is risky, since the intermediate and final outputs can be too large to store in the notebook instance.\n",
    "- **Region**. Make sure that your region is a region included in AWS Batch.\n",
    "\n",
    "__Note:__ Best practices are to make sure your working directory (`workDir`) and output directory (`outdir`) are **different**! AWS Batch creates temporary files in the working directory within your bucket that do take up space. So once your pipeline has completed successfully, feel free to delete the temporary files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656a114-d2ca-476b-ace8-f0fee93ecb03",
   "metadata": {},
   "source": [
    "### Download and **Test** nf-core/methylseq using AWS Batch\n",
    "\n",
    "The `test` profile (`-profile test`) uses a small dataset allowing you to ensure the workflow works with your config file without long run times. Ensure you include:\n",
    "- Version of the nf-core tool [-r]\n",
    "- Location of the config file [-c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f683b5-62a2-44f6-92c8-f27d46863124",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow run nf-core/methylseq -r 2.4.0 -profile test,aws -c ./docs/aws-batch.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599af349-877b-4f47-b2ab-fd3bc8f944be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-pencil\" aria-hidden=\"true\"></i>\n",
    "<b>Note</b>: The <code>preseq</code> process may failed but ignored in the pipeline. This won't affect the output results. The preseq package is aimed at predicting and estimating the complexity of a genomic sequencing library, equivalent to predicting and estimating the number of redundant reads from a given sequencing depth and how many will be expected from additional sequencing using an initial sequencing experiment.\n",
    "    </div>\n",
    "\n",
    "This nf-core/methyseq test profile takes about 20 minutes to finish. When compared with the test profile running time (about 3 minutes) from Tutorial 3, we can see that there is extra time needed for Nextflow to talk to Google Batch, Cloud Storage, and VMs. It is not worthwhile for a small dataset, but this time difference can be ignored when running large datasets that need more computational resources. In other words, Google Batch works well for coarse-grained workloads i.e. long-running jobs. Itâ€™s not suggested to use this feature for pipelines spawning many short-lived tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ecdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the bucket for the next part\n",
    "! aws s3 rm --recursive s3://nextflow-bucket-test/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01c9ed-e2b7-48af-b5ef-1cd27a92e0c3",
   "metadata": {},
   "source": [
    "## An Example of a Real World Dataset<a name=\"REAL\"/>\n",
    "\n",
    "1. Install SRA-tools\n",
    "2. Download the Data\n",
    "3. Modify the config file\n",
    "4. Run the job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4fb16-a862-480e-9002-8ee9861202db",
   "metadata": {},
   "source": [
    "#### Install SRA-tools\n",
    "\n",
    "The **[SRA Toolkit](https://github.com/ncbi/sra-tools/wiki)** and SDK from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives. Here we use `mamba` to install `sra-tools`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314272a-5980-4c46-9217-838de16313d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mamba install -c bioconda sra-tools=3.0.5 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db87bc0-f745-4aba-aecd-5c4606238594",
   "metadata": {},
   "source": [
    "#### Download the Data\n",
    "\n",
    "The data was from [Molaro, Antoine, et al. Cell 146.6 (2011): 1029-1041](https://www.sciencedirect.com/science/article/pii/S0092867411009421) and [Laurent, Louise, et al. \" Genome research 20.3 (2010): 320-331](https://genome.cshlp.org/content/20/3/320.full). During germ cell and preimplantation development, mammalian cells undergo nearly complete reprogramming of DNA methylation patterns. The studies profiled the methylomes of human and chimp sperm as a basis for comparison to methylation patterns of embryonic stem cells (ESCs).   \n",
    "<img src=\"images/4_data_graph.jpg\" width=\"300\" />\n",
    "\n",
    "We use one sample from human sperm and one sample from ESCs as examples to demonstrate the workflow here.\n",
    "\n",
    "Use `fasterq-dump` to download data from SRA using accession numbers. The data will be store at `Tutorial_4/sra_download`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e74cb8-1ffc-484f-8b26-4c11703cbc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "! fasterq-dump --threads 4 --progress SRR306435 SRR033942 -O Tutorial_4/sra_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c0fd0-0309-4451-96ae-ea4156a5b734",
   "metadata": {},
   "source": [
    "Remove the temporary output directory from running `fasterq-dump`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5a465-a3a7-4968-84c4-f6d7bed30dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf fasterq.tmp.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152126a-9c1a-4eff-a8ea-e51f45bb9a44",
   "metadata": {},
   "source": [
    "Compress the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc1fcd-7496-4186-9b36-94678283e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faster way to compress the files\n",
    "! pigz Tutorial_4/sra_download/SRR*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775f2b8-83a4-4e96-aa6b-64108377f22e",
   "metadata": {},
   "source": [
    "#### Create a samplesheet (located in Tutorial_3) to provide all sample information\n",
    "\n",
    "**Format:**    \n",
    "sample, fastq1, fastq2    \n",
    "sample1,sample1_R1.fastq,sample1_R2.fastq    \n",
    "control1,control1_R1.fastq,control1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27799ac8-6fd2-4c2e-abb6-af9c81bbef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame by lists of dicts.\n",
    "import pandas as pd\n",
    " \n",
    "# Initialize data to lists.\n",
    "samples = [{'sample': 'SRR033942', 'fastq_1': 'Tutorial_4/sra_download/SRR033942_1.fastq.gz', 'fastq_2': 'Tutorial_4/sra_download/SRR033942_2.fastq.gz'},\n",
    "        {'sample': 'SRR306435', 'fastq_1': 'Tutorial_4/sra_download/SRR306435_1.fastq.gz', 'fastq_2': 'Tutorial_4/sra_download/SRR306435_2.fastq.gz'}\n",
    "       ]\n",
    " \n",
    "# Creates DataFrame.\n",
    "df2 = pd.DataFrame(samples)\n",
    " \n",
    "# Print the data\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b35d6-c886-410f-a040-371d26c4c3e0",
   "metadata": {},
   "source": [
    "Export dataframe to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bf795-bdd9-486e-ae8e-2ae4fa6c25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('Tutorial_4/samplesheet.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b000a077-b203-4b31-949b-39fb4b56fc0d",
   "metadata": {},
   "source": [
    "#### Run methylseq using AWS Batch\n",
    "\n",
    "If not defined in the config file, you can always use command line parameters:\n",
    "- `-r` pipeline version\n",
    "- `-profile` profile to use ('gcb' was defined in docs/aws-batch.config)\n",
    "- `-c` config file\n",
    "- `--fasta` the reference sequences, usually the reference genome. Here we use human assembly GRCh38 as the reference genome. \n",
    "- `--clip_r1` instructs Trim Galore to remove certein number of bps from the 5' end of read 1\n",
    "- `--tracedir` defines a local diretory to save the pipeline information\n",
    "\n",
    "There will be some pipeline information saved to the default `nextflow_output` directory. So please make sure the directory is empty before running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15f673-f29e-4d75-a6b4-5801ca4a1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf Tutorial_4/methyseq_sperm\n",
    "\n",
    "! nextflow run nf-core/methylseq \\\n",
    "    -profile aws \\\n",
    "    -r 2.6.0 \\\n",
    "    -c ./docs/aws-batch.config  \\\n",
    "    --input 'Tutorial_4/samplesheet.csv' \\\n",
    "    --fasta s3://nigms-sandbox/references/Homo_sapiens/NCBI/GRCh38/Sequence/BismarkIndex/genome.fa \\\n",
    "    --clip_r1 2 \\\n",
    "    --tracedir 'Tutorial_4/methyseq_sperm/pipeline_info' \\\n",
    "    -resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695dbca-104a-4bab-b0bc-393c0b3d547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the working file diretory\n",
    "! aws s3 rm --recursive s3://nextflow-bucket-test/nextflow_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02012eb4-cc48-49b8-a7df-5c270b776734",
   "metadata": {},
   "source": [
    "#### Check to see if files are in your output directory bucket\n",
    "\n",
    "The output files should be saved in your bucket's `nextflow_output` directory. You can list the results directory to see the file structures. You can also copy the files to your local directory to view them. For example, the MultiQC report file is located at `s3://nextflow-bucket-test/nextflow_output/multiqc/bismark/multiqc_report.html`. Let's copy and view it using the commands below:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "<b>Note:</b> Please <b>replace</b> the \"nextflow-bucket-test\" in commands below with your own bucket name.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093cf8e2-eb12-46fd-9fd3-ca0ac5e35a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the output files/directories in the results folder\n",
    "! aws s3 ls s3://nextflow-bucket-test/nextflow_output/\n",
    "\n",
    "# Copy the multiQC output multiqc_report.html to local notebook:\n",
    "! aws s3 cp s3://nextflow-bucket-test/nextflow_output/multiqc/bismark/multiqc_report.html .\n",
    "\n",
    "# View the MultiQC output HTML file:\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='multiqc_report.html', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad5eda-2318-4ec0-a353-0dac3955bbf7",
   "metadata": {},
   "source": [
    "There are two files (`execution_timeline.html` and `execution_report.html`) about the pipeline running information will be saved in the null/pipeline_info directory locally in the notebook, which can provide detailed information about the running time for each process and the their resource usages. This can provide more insights for potential optimizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a760c-8805-4354-a2e6-f4a317d3f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the file names listed in here\n",
    "! ls null/pipeline_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af3cc4-caf1-4d4f-8133-d307dbaa2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='null/pipeline_info/execution_timeline_2025-02-04_19-39-39.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49deb8-3b8d-4554-9d0c-6a6a9a9aa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='null/pipeline_info/execution_report_2025-02-04_19-39-39.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4052257-cebd-4c6d-b942-f03a40269540",
   "metadata": {},
   "source": [
    "### Configuration of a Full-scale Dataset - Troubleshooting\n",
    "\n",
    "For a full-scale WGBS study, the sequencing data size can range from several hundred GBs to several TBs. For example, the data we downloaded in this tutorial: GSE30340 and GSE19418, both have many runs with the size add up to several hundred GBs. Given the large data files, the storage and memory can become an issue when running the pipeline as instructed in this tutorial. \n",
    "\n",
    "#### Download the data\n",
    "\n",
    "There are several options that we can use to download the data:\n",
    "1. Download the data in a notebook. You need to make sure that the disk size you assigned to the notebook is enough for the data that you want to download. Also, when you use `prefetch` from SRA toolkit, there is a default maximum download-size of 5G; you will need to increase that limit.  \n",
    "2. Cloud Data Delivery Service. SRA has created a cloud data delivery service to deliver the source files and other file types from NCBI cold storage buckets to individual data consumers' buckets in AWS and GCP. This service is provided for both public and authorized access (dbGaP) data. [More detailed information here](https://www.ncbi.nlm.nih.gov/sra/docs/data-delivery/).\n",
    "3. Upload to the S3 bucket directly. You can upload the data to the Amazon S3 bucket directly from your local computer, HPC, or service server using the `aws s3` tool.\n",
    "\n",
    "#### Troubleshooting the nf-core pipeline\n",
    "\n",
    "If the nf-core pipeline does not complete successfully, you can refer to the [troubleshooting](https://nf-co.re/usage/troubleshooting) page that nf-core provided for more information. For our tutorial here, the most likely reasons that the pipeline fails are:\n",
    "- service account is not set up correctly\n",
    "- file paths are not correct\n",
    "- memory or storage issues for large dataset\n",
    "\n",
    "If you have a command exit status of 104, 134, 137, 139, 143, 247, the most probable cause is an \"out of memory\" issue. To solve the memory issue, you need to increase the memory limit in the configuration file for the process that fails. For example:   \n",
    "```\n",
    "profiles {\n",
    "    aws {\n",
    "        process {\n",
    "            withName: qualimap {\n",
    "                instanceType = 'ml.m5.2xlarge'  // Or another suitable instance type\n",
    "                cpus = 16\n",
    "                memory = 64.GB\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In AWS, the memory is also limited by the [notebook instance type](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html) you select to run the process. For example, if you choose `ml.m5.2xlarge` then the memory is limited to 32GB. You can change the machine types to increase the memory. \n",
    "\n",
    "#### Optimize nf-core/methylseq configuration\n",
    "\n",
    "The nf-core/methylseq workflow contains multiple processes, and the requirements of computational and memory resources for each process vary a lot. For better performance or billing purposes, you can change the configuration for each process. You can check the default settings for each process at the pipeline's [base.config](https://github.com/nf-core/methylseq/blob/master/conf/base.config) file.  \n",
    "As an example of running a 12 sample WGBS data, [docs/optimization_example.config](docs/optimization_example.config) was the config file that finish processing these 24 fastq files (pair-end, averge size 325M reads per fastq file, ) using Google Batch in less than 30 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8141c9ff",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This Jupyter Notebook demonstrated how to leverage AWS Batch within the Nextflow framework for efficient WGBS data analysis using the nf-core/methylseq pipeline.  We addressed the limitations of processing large datasets on single virtual machines by outlining the necessary steps for setting up a Nextflow service account, configuring an Amazon S3 bucket, and creating a Nextflow config file tailored for AWS Batch execution. The tutorial progressed from a test run with a small dataset to a more realistic example using real-world SRA data, highlighting the process of data acquisition, samplesheet creation, and pipeline execution.  Finally, we provided crucial guidance on troubleshooting issues common to large-scale datasets, including memory management, and offered strategies for optimizing pipeline configuration for enhanced performance and cost-effectiveness.  The successful completion of this notebook empowers users to efficiently analyze sizable WGBS datasets using the scalability and resource management capabilities of AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71539336-703c-4c71-ad0d-b6aabae94189",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "**Remember to move to the next notebook or shut down your instance if you are finished.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
